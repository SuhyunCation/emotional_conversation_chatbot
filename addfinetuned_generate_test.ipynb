{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hkyoon95/anaconda3/envs/koalpaca/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-17 02:47:39,662] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  6.92s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import torch\n",
    "from transformers import GenerationConfig, AutoModelForQuestionAnswering, AutoTokenizer, pipeline, AutoModelForCausalLM\n",
    "# 추가학습한 koalpaca inference\n",
    "device = f\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "completion_model_name = '/hdd/hkyoon95/polyglot-5.8b-koalpaca-emotion'\n",
    "# completion_model_name = 'beomi/KoAlpaca-KoRWKV-6B'\n",
    "generation_config = GenerationConfig.from_pretrained(completion_model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(completion_model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    completion_model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    ").to(device=device, non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력:  오늘 회사에서 일을 망쳤어\n",
      "출력:   회사에서 일을 망치셨군요. 어떤 상황이었나요?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력:  발표를 너무 못했어\n",
      "출력:   발표를 잘 못 하셔서 속상하셨겠어요.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력:  같이 준비한 동료들한테 미안해\n",
      "출력:   준비한 동료들에게 미안한 마음이 크시군요.\n"
     ]
    }
   ],
   "source": [
    "def gen(prompt=None, max_new_tokens=128, temperature=0.3):\n",
    "    # max_new_tokens: generate token 제한 개수\n",
    "    # temperature: sampling method hyperparameter 높을수록 random하게 generate할 확률이 증가함\n",
    "    user_input = []\n",
    "    output = []\n",
    "    for i in range(3):\n",
    "        if i == 0:\n",
    "            text = input(\"Please enter something: \")\n",
    "            x = \"### 질문: {}\\n\\n### 답변:\".format(text)\n",
    "            user_input.append(text)\n",
    "        else:\n",
    "            text = input(\"Please enter something: \")\n",
    "            user_input.append(text)\n",
    "            x = x + gen_text[0].replace(x, '').split('###')[0].strip('\\n').strip(' ') \\\n",
    "                    + '\\n\\n### 질문: {}\\n\\n### 답변: '.format(text)\n",
    "        input_ids = tokenizer.encode(x, return_tensors=\"pt\").to('cuda:0')\n",
    "        gen_tokens = model.generate(\n",
    "            input_ids, \n",
    "            max_new_tokens=max_new_tokens,\n",
    "            num_return_sequences=1, \n",
    "            temperature=temperature,\n",
    "            no_repeat_ngram_size=6,\n",
    "            do_sample=True,\n",
    "        )\n",
    "        gen_text = tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)\n",
    "        # return gen_text\n",
    "        print('입력: ', text)\n",
    "        print('출력: ', gen_text[0].replace(x, '').split('###')[0])\n",
    "        output.append(gen_text[0].replace(x, '').split('###')[0].strip('\\n').strip(' '))\n",
    "        \n",
    "    return user_input, output\n",
    "user_input, output = gen(temperature=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(user_input)):\n",
    "    print('입력: {}'.format(user_input[i]))\n",
    "    print('출력: {}'.format(output[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test original koalpaca\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline('text-generation', model='beomi/KoAlpaca-KoRWKV-6B', device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(x, temp):\n",
    "    a = pipe(\n",
    "        f'### 질문: {x}\\n\\n### 답변:', \n",
    "        max_new_tokens=1024, \n",
    "        return_full_text=False, \n",
    "        do_sample=True,\n",
    "        # top_p=0.9,\n",
    "        temperature=temp,\n",
    "        early_stopping=True,\n",
    "        repetition_penalty=1.2,\n",
    "    )\n",
    "    print(a[0]['generated_text'])\n",
    "gen('삼촌이 추석 때 취업하라 그래서 짜증나', temp=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "같이 먹을래?\n"
     ]
    }
   ],
   "source": [
    "prompt = '오늘 점심 뭐 먹을까?'\n",
    "generated_text = gen(user_input = prompt, prompt = '다음 문장에 공감 또는 대답해주세요', temperature=0.3)\n",
    "print(generated_text.split('###')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 좋은 선택이에요.\n"
     ]
    }
   ],
   "source": [
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"### Instruction(명령어):\\n### Input(입력):오늘 점심 뭐 먹을까?\\n### Response(응답):오늘은 짬뽕이 어떠신가요.\\n### Instruction(명령어):\\n{instruction}\\n\\n### Input(입력):\\n{input}\\n\\n### Response(응답):\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"### Instruction(명령어):\\n{instruction}\\n\\n### Response(응답):\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "def gen(user_input=None, prompt, max_new_tokens=128, temperature=0.3):\n",
    "    if user_input:\n",
    "        x = PROMPT_DICT['prompt_input'].format(instruction=prompt, input=user_input)\n",
    "    else:\n",
    "        x = PROMPT_DICT['prompt_no_input'].format(instruction=prompt)\n",
    "    \n",
    "    input_ids = tokenizer.encode(x, return_tensors=\"pt\").to('cuda:0')\n",
    "    gen_tokens = model.generate(\n",
    "        input_ids, \n",
    "        max_new_tokens=max_new_tokens, \n",
    "        num_return_sequences=1, \n",
    "        temperature=temperature,\n",
    "        no_repeat_ngram_size=6,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    gen_text = tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)\n",
    "    # return gen_text\n",
    "    return gen_text[0].replace(x, '')\n",
    "prompt = '좋은 거 같아'\n",
    "generated_text = gen(user_input = prompt, prompt = '', temperature=1.1)\n",
    "print(generated_text.split('###')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"추석 때 취업 관련한 언급이 불편하게 느껴질 수 있다는 것에 공감합니다.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"추석 때 취업 관련한 언급이 불편하게 느껴질 수 있겠네요. 스트레스 받는 상황이겠지만, 삼촌과의 대화에서 더 나은 대응을 할 수 있는 방법을 모색해보는 것이 좋겠습니다.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"추석 때 취업 얘기 꺼내면 짜증나죠. 나는 아직 준비가 안 됐는데.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"추석 때 취업 관련한 언급이 불편하게 느껴질 수 있겠네요. 스트레스 받지 마시고, 즐거운 추석 연휴 보내시길 바랍니다.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"추석 때 취업 관련한 언급이 불편하게 느껴질 수 있겠네요. 스트레스 받는 상황이겠지만, 삼촌과의 대화에서 더 나은 대응을 할 수 있는 방법을 모색해보는 것이 좋겠습니다.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"추석 때 취업 관련한 언급이 불편하게 느껴질 수 있겠네요. 스트레스 받지 마시고, 즐거운 추석 연휴 보내시길 바랍니다.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"추석 때 취업 관련한 언급이 불편하게 느껴질 수 있겠네요. 스트레스 받는 상황이겠지만, 삼촌과의 대화에서 더 나은 대응을 할 수 있는 방법을 모색해보는 것이 좋겠습니다.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"추석 때 취업 관련한 언급이 불편하게 느껴질 수 있겠네요. 스트레스 받는 상황이겠지만, 삼촌과의 대화에서 더 나은 대응을 할 수 있는 방법을 모색해보는 것이 좋겠습니다.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"추석 때 취업 관련한 언급이 불편하게 느껴질 수 있다는 것에 공감합니다.\"\n",
      "\n",
      "\"추석 때 취업 관련한 언급이 불편하게 느껴질 수 있겠네요. 스트레스 받지 마시고, 즐거운 추석 연휴 보내시길 바랍니다.\"\n"
     ]
    }
   ],
   "source": [
    "prompt = \"추석에 삼촌이 와서 취업 했냐고 물어보면 진짜 짜증나\"\n",
    "for i in range(10):\n",
    "    generated_text = gen(user_input = '내 말에 공감해줘', prompt = prompt, temperature=0.1)\n",
    "    print(generated_text.split('###')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
